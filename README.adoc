= aioresilience - Fault Tolerance Library for Asyncio
:author: aioresilience contributors
:icons: font
:toc: macro
:numbered:
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]

image:https://img.shields.io/badge/python-3.8+-blue.svg["Python 3.8+", link="https://www.python.org/downloads/"]
image:https://img.shields.io/badge/License-MIT-yellow.svg["MIT License", link="https://opensource.org/licenses/MIT"]
image:https://img.shields.io/badge/type__hints-PEP%20484-brightgreen.svg["Type Hints", link="https://www.python.org/dev/peps/pep-0484/"]
image:https://img.shields.io/badge/code%20style-black-000000.svg["Code Style: Black", link="https://github.com/psf/black"]

toc::[]

== Introduction

aioresilience is a lightweight fault tolerance library designed for Python's asyncio ecosystem.
aioresilience provides higher-order functions (decorators) to enhance any async function, 
lambda expression or coroutine with a Circuit Breaker, Rate Limiter, Load Shedder, Backpressure Manager or Adaptive Concurrency Limiter.
You can stack more than one decorator on any async function or coroutine.
The advantage is that you have the choice to select the decorators you need and nothing else.

aioresilience requires Python 3.8+.

[source,python]
----
from aioresilience import CircuitBreaker, RateLimiter, LoadShedder

# Create a CircuitBreaker with default configuration
circuit_breaker = CircuitBreaker(name="backendService", failure_threshold=5)

# Create a RateLimiter with local in-memory storage
rate_limiter = RateLimiter(service_name="backendService")

# Create a LoadShedder with default configuration
load_shedder = LoadShedder(max_requests=1000)

async def backend_call():
    return await backend_service.do_something()

# Decorate your call to backend_service.do_something()
# with a Circuit Breaker, Rate Limiter and Load Shedder
from aioresilience import circuit_breaker, with_load_shedding

@circuit_breaker("backendService", failure_threshold=5)
@with_load_shedding(load_shedder, priority="normal")
async def decorated_call(user_id: str):
    # Check rate limit
    if await rate_limiter.check_rate_limit(user_id, "100/minute"):
        return await backend_service.do_something()
    else:
        raise Exception("Rate limit exceeded")

# Execute the decorated function and handle exceptions
try:
    result = await decorated_call("user_123")
except Exception as e:
    result = "Fallback value"

# When you don't want to decorate your function,
# but just execute it and protect the call by a CircuitBreaker
result = await circuit_breaker.call(backend_call)
----

NOTE: With aioresilience you don't have to go all-in, you can
https://pypi.org/project/aioresilience/[*pick what you need*].

== Documentation

Complete documentation is available in this README and through Python docstrings.

== Installation

=== Basic Installation

[source,bash]
----
pip install aioresilience
----

=== With Optional Features

[source,bash]
----
# Redis-based distributed rate limiting
pip install aioresilience[redis]

# System metrics monitoring (CPU/memory)
pip install aioresilience[system]

# FastAPI integration
pip install aioresilience[fastapi]

# Everything
pip install aioresilience[all]
----

== Overview

aioresilience provides several core modules:

* *aioresilience-circuitbreaker*: Circuit breaking with state management
* *aioresilience-ratelimiter*: Rate limiting (local and distributed)
* *aioresilience-loadshedding*: Load shedding (basic and system-aware)
* *aioresilience-backpressure*: Backpressure management with water marks
* *aioresilience-adaptive*: Adaptive concurrency limiting with AIMD algorithm

There are also add-on modules for FastAPI, and planned support for Flask, Django, and other frameworks.

NOTE: All core modules are included in the base package. Use optional dependencies (`redis`, `psutil`, `fastapi`) to enable additional features.

TIP: For all features install with `pip install aioresilience[all]`.

== Resilience Patterns

[cols="<.<*", options="header"]
|===
|Name |How Does It Work? |Description

|*Circuit Breaker*
|Temporarily blocks possible failures
|When a system is seriously struggling, failing fast is better than making clients wait. 
Prevents cascading failures by monitoring error rates and opening the circuit when thresholds are exceeded.

|*Rate Limiter*
|Limits executions per time period
|Control the rate of incoming requests with configurable windows (second, minute, hour, day).
Supports both local (in-memory) and distributed (Redis) backends.

|*Load Shedder*
|Rejects requests under high load
|Protect your system by rejecting new requests when load exceeds thresholds.
Supports request-count-based and system-metric-based (CPU/memory) shedding.

|*Backpressure Manager*
|Controls flow in async pipelines
|Signal upstream components to slow down when downstream is overloaded.
Uses water marks (high/low) for graceful flow control.

|*Adaptive Concurrency*
|Auto-adjusts concurrency limits
|Dynamically adjust concurrency based on success rate using AIMD algorithm.
Similar to TCP congestion control - additive increase, multiplicative decrease.

|===

_Above table is inspired by https://github.com/App-vNext/Polly#resilience-policies[Polly: resilience policies] 
and https://github.com/resilience4j/resilience4j[resilience4j]._

== Usage Examples

[[circuitbreaker-examples]]
=== Circuit Breaker

The following example shows how to decorate an async function with a Circuit Breaker and how to handle state transitions.

[source,python]
----
import asyncio
from aioresilience import CircuitBreaker, circuit_breaker

# Simulates a Backend Service
class BackendService:
    async def do_something(self):
        # Simulate API call
        async with httpx.AsyncClient() as client:
            response = await client.get("https://api.example.com/data")
            return response.json()

backend_service = BackendService()

# Create a CircuitBreaker with custom configuration
circuit = CircuitBreaker(
    name="backendName",
    failure_threshold=5,      # Open after 5 consecutive failures
    recovery_timeout=60.0,    # Wait 60 seconds before trying half-open
    success_threshold=2       # Need 2 successes to close from half-open
)

# Decorate your call to BackendService.do_something()
async def call_backend():
    if await circuit.can_execute():
        try:
            result = await circuit.call(backend_service.do_something)
            return result
        except Exception as e:
            # Circuit breaker automatically tracks the failure
            raise
    else:
        raise Exception("Circuit breaker is OPEN")

# Or use the decorator pattern
@circuit_breaker("backendName", failure_threshold=5)
async def decorated_backend_call():
    return await backend_service.do_something()

# Execute with fallback
async def call_with_fallback():
    try:
        result = await decorated_backend_call()
        return result
    except Exception:
        return {"data": "fallback_value"}

# When you don't want to decorate your function
result = await circuit.call(backend_service.do_something)
----

==== Circuit Breaker States

The circuit breaker has three states:

* *CLOSED*: Normal operation, requests pass through
* *OPEN*: Failure threshold exceeded, requests fail fast
* *HALF_OPEN*: Testing recovery, limited requests allowed

==== Monitoring Circuit Breaker

You can monitor circuit breaker state and metrics:

[source,python]
----
# Get current state
state = circuit.get_state()
print(f"Circuit state: {state}")

# Get detailed metrics
metrics = circuit.get_metrics()
print(f"Total requests: {metrics['total_requests']}")
print(f"Failed requests: {metrics['failed_requests']}")
print(f"Failure rate: {metrics['failure_rate']:.2%}")

# Access global circuit breaker manager
from aioresilience import get_circuit_breaker, get_all_circuit_metrics

# Get or create a circuit breaker
backend_circuit = get_circuit_breaker("backend", failure_threshold=3)

# Get metrics for all circuit breakers
all_metrics = get_all_circuit_metrics()
for name, metrics in all_metrics.items():
    print(f"{name}: {metrics['state']}")
----

[[ratelimiter-examples]]
=== Rate Limiter

The following example shows how to restrict the calling rate to not be higher than 10 requests per second.

[source,python]
----
from aioresilience import RateLimiter

# Create a RateLimiter (local/in-memory)
rate_limiter = RateLimiter(service_name="backendName")

# Check rate limit for a specific key (e.g., user ID)
async def handle_request(user_id: str):
    if await rate_limiter.check_rate_limit(user_id, "10/second"):
        # Request is within rate limit
        return await backend_service.do_something()
    else:
        # Rate limit exceeded
        raise Exception("Rate limit exceeded")

# First call succeeds
try:
    result = await handle_request("user_123")
    print("Request successful")
except Exception as e:
    print(f"Request failed: {e}")

# If you make 11 calls in one second, the 11th will fail
import asyncio

async def test_rate_limit():
    for i in range(11):
        try:
            result = await handle_request("user_123")
            print(f"Call {i+1} successful")
        except Exception as e:
            print(f"Call {i+1} failed: {e}")

asyncio.run(test_rate_limit())
----

==== Rate Limit Formats

aioresilience supports multiple time periods:

* `"10/second"` - 10 requests per second
* `"100/minute"` - 100 requests per minute
* `"1000/hour"` - 1000 requests per hour
* `"10000/day"` - 10000 requests per day

==== Distributed Rate Limiting with Redis

For multi-instance applications, use Redis-based distributed rate limiting:

[source,python]
----
from aioresilience.rate_limiting import RedisRateLimiter

# Create a Redis-backed rate limiter
rate_limiter = RedisRateLimiter(service_name="backendName")
await rate_limiter.init_redis("redis://localhost:6379")

# Use the same API - now shared across all instances
if await rate_limiter.check_rate_limit("user_123", "1000/hour"):
    result = await backend_service.do_something()
else:
    raise Exception("Rate limit exceeded")

# Don't forget to close the connection when done
await rate_limiter.close()
----

NOTE: Redis rate limiter uses a sliding window algorithm with sorted sets for accurate distributed rate limiting.

[[loadshedding-examples]]
=== Load Shedding

There are two load shedding implementations.

==== BasicLoadShedder

The following example shows how to shed load based on request count:

[source,python]
----
from aioresilience import LoadShedder

# Create a LoadShedder with request count limits
load_shedder = LoadShedder(
    max_requests=1000,       # Maximum concurrent requests
    max_queue_depth=500      # Maximum queue depth
)

# Use in your request handler
async def handle_request():
    if await load_shedder.acquire():
        try:
            # Process the request
            result = await backend_service.do_something()
            return result
        finally:
            await load_shedder.release()
    else:
        # Load shedding - reject request
        raise Exception("Service overloaded")

# Or use the decorator
from aioresilience import with_load_shedding

@with_load_shedding(load_shedder, priority="normal")
async def process_request():
    return await backend_service.do_something()
----

==== SystemLoadShedder

The following example shows how to shed load based on system metrics (CPU and memory):

[source,python]
----
from aioresilience.load_shedding import SystemLoadShedder

# Create a system-aware load shedder
load_shedder = SystemLoadShedder(
    max_requests=1000,
    cpu_threshold=85.0,      # Shed load if CPU > 85%
    memory_threshold=85.0    # Shed load if memory > 85%
)

# Use the same API as BasicLoadShedder
async def handle_request():
    if await load_shedder.acquire(priority="normal"):
        try:
            result = await backend_service.do_something()
            return result
        finally:
            await load_shedder.release()
    else:
        raise Exception("Service overloaded - high system load")

# High priority requests can bypass some checks
if await load_shedder.acquire(priority="high"):
    # High priority request processing
    pass
----

NOTE: SystemLoadShedder requires the `psutil` package. Install with `pip install aioresilience[system]`.

[[backpressure-examples]]
=== Backpressure Management

Control flow in async processing pipelines using water marks:

[source,python]
----
from aioresilience import BackpressureManager

# Create a backpressure manager
backpressure = BackpressureManager(
    max_pending=1000,        # Hard limit on pending items
    high_water_mark=800,     # Start applying backpressure
    low_water_mark=200       # Stop applying backpressure
)

# Use in async pipeline
async def process_stream(items):
    for item in items:
        # Try to acquire slot (with timeout)
        if await backpressure.acquire(timeout=5.0):
            try:
                await process_item(item)
            finally:
                await backpressure.release()
        else:
            # Backpressure timeout - item rejected
            logger.warning(f"Item rejected due to backpressure")

# Or use the decorator
from aioresilience import with_backpressure

@with_backpressure(backpressure, timeout=5.0)
async def process_item(item):
    # Your processing logic
    await asyncio.sleep(0.1)
    return item
----

[[adaptive-examples]]
=== Adaptive Concurrency Limiting

Automatically adjust concurrency limits based on success rate:

[source,python]
----
from aioresilience import AdaptiveConcurrencyLimiter

# Create an adaptive limiter with AIMD algorithm
limiter = AdaptiveConcurrencyLimiter(
    initial_limit=100,       # Starting concurrency
    min_limit=10,            # Minimum concurrency
    max_limit=1000,          # Maximum concurrency
    increase_rate=1.0,       # Additive increase
    decrease_factor=0.9      # Multiplicative decrease
)

# Use in your request handler
async def handle_request():
    if await limiter.acquire():
        try:
            result = await backend_service.do_something()
            # Report success
            await limiter.release(success=True)
            return result
        except Exception as e:
            # Report failure
            await limiter.release(success=False)
            raise
    else:
        raise Exception("Concurrency limit reached")

# Check current statistics
stats = limiter.get_stats()
print(f"Current limit: {stats['current_limit']}")
print(f"Active requests: {stats['active_count']}")
----

NOTE: The AIMD algorithm increases the limit linearly on success and decreases it exponentially on failure, similar to TCP congestion control.

[[fastapi-integration]]
== FastAPI Integration

aioresilience provides seamless integration with FastAPI applications.

=== Load Shedding Middleware

[source,python]
----
from fastapi import FastAPI
from aioresilience import LoadShedder
from aioresilience.integrations.fastapi import LoadSheddingMiddleware

app = FastAPI()

# Create load shedder
load_shedder = LoadShedder(max_requests=1000)

# Add middleware
app.add_middleware(LoadSheddingMiddleware, load_shedder=load_shedder)

@app.get("/api/data")
async def get_data():
    return {"data": "your data here"}

# Requests will be automatically shed when overloaded
# Returns 503 Service Unavailable with Retry-After header
----

=== Rate Limiting Dependency

[source,python]
----
from fastapi import FastAPI, Depends
from aioresilience import RateLimiter
from aioresilience.integrations.fastapi import rate_limit_dependency

app = FastAPI()
rate_limiter = RateLimiter(service_name="api")

@app.get(
    "/api/data",
    dependencies=[Depends(rate_limit_dependency(rate_limiter, "100/minute"))]
)
async def get_data():
    return {"data": "your data here"}

# Rate limiting is automatically applied per client IP
# Returns 429 Too Many Requests when limit exceeded
----

=== Custom Client IP Extraction

[source,python]
----
from fastapi import Request
from aioresilience.integrations.fastapi import get_client_ip

@app.middleware("http")
async def custom_middleware(request: Request, call_next):
    client_ip = get_client_ip(request)
    # Supports X-Forwarded-For and X-Real-IP headers
    logger.info(f"Request from {client_ip}")
    response = await call_next(request)
    return response
----

[[events]]
== Event Consumption

Circuit Breaker components emit events that can be consumed for logging, monitoring, and metrics.

=== Circuit Breaker Events

A `CircuitBreakerEvent` can be a state transition, successful call, recorded error, or circuit reset.

[source,python]
----
from aioresilience import CircuitBreaker
import logging

logger = logging.getLogger(__name__)

circuit = CircuitBreaker(name="backend", failure_threshold=5)

# Monitor circuit breaker metrics
async def monitor_circuit():
    while True:
        metrics = circuit.get_metrics()
        logger.info(f"Circuit {metrics['name']}: "
                   f"state={metrics['state']}, "
                   f"failures={metrics['consecutive_failures']}, "
                   f"success_rate={metrics['failure_rate']:.2%}")
        await asyncio.sleep(10)

# Start monitoring
asyncio.create_task(monitor_circuit())
----

=== Load Shedder Metrics

[source,python]
----
# Get load shedder statistics
stats = load_shedder.get_stats()
print(f"Active requests: {stats['active_requests']}")
print(f"Total shed: {stats['total_shed']}")
print(f"Load level: {stats['load_level']}")
print(f"Utilization: {stats['utilization']:.1f}%")
----

=== Rate Limiter Metrics

[source,python]
----
# Get rate limiter statistics
stats = rate_limiter.get_stats()
print(f"Active limiters: {stats['active_limiters']}")
print(f"Type: {stats['type']}")
----

== Architecture

aioresilience follows a modular architecture with minimal required dependencies:

[source]
----
aioresilience/
├── circuit_breaker.py        # No external dependencies
├── backpressure.py            # No external dependencies
├── adaptive_concurrency.py    # No external dependencies
├── rate_limiting/
│   ├── __init__.py
│   ├── local.py              # Requires: aiolimiter
│   └── redis.py              # Requires: redis (optional)
├── load_shedding/
│   ├── __init__.py
│   ├── basic.py              # No external dependencies
│   └── system.py             # Requires: psutil (optional)
└── integrations/
    ├── __init__.py
    └── fastapi.py            # Requires: fastapi (optional)
----

=== Design Philosophy

1. *Async-First*: Built from the ground up for Python's asyncio
2. *Fail-Safe Defaults*: Components fail open to preserve availability
3. *Modular*: Use only what you need, no unnecessary dependencies
4. *Type-Safe*: Full type hints (PEP 484) for better IDE support
5. *Production-Ready*: Thread-safe with proper async locking
6. *Observable*: Rich metrics and statistics for monitoring

=== Comparison with Other Libraries

[cols="<,^,^,^", options="header"]
|===
|Feature |aioresilience |pybreaker |circuitbreaker

|Async-native
|✓
|✗
|✗

|Type hints
|✓
|Partial
|✗

|Multiple patterns
|✓
|✗
|✗

|Modular design
|✓
|✗
|✗

|Distributed rate limiting
|✓
|✗
|✗

|Load shedding
|✓
|✗
|✗

|Metrics & monitoring
|✓
|Basic
|Basic

|===

== Performance

Overhead measurements on a typical development machine:

* *Circuit Breaker*: ~2-5µs per call
* *Rate Limiter (Local)*: ~10-20µs per check
* *Rate Limiter (Redis)*: ~1-2ms per check (network dependent)
* *Load Shedder (Basic)*: ~1-3µs per acquire/release
* *Load Shedder (System)*: ~5-10µs per acquire/release (includes system metrics)
* *Backpressure Manager*: ~2-4µs per acquire/release
* *Adaptive Concurrency*: ~2-5µs per acquire/release

NOTE: These are rough estimates. Actual performance depends on your hardware and workload.

== Roadmap

Planned features for future releases:

* [ ] Retry policies with exponential backoff
* [ ] Bulkhead pattern for resource isolation
* [ ] Time limiters / deadlines
* [ ] Fallback mechanisms (enhanced)
* [ ] Django integration
* [ ] Flask integration  
* [ ] Sanic integration
* [ ] Prometheus metrics exporter
* [ ] OpenTelemetry integration
* [ ] Grafana dashboard templates
* [ ] Enhanced event streaming
* [ ] WebSocket support

== Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

For major changes, please open an issue first to discuss what you would like to change.

=== Development Setup

[source,bash]
----
# Clone the repository
git clone https://github.com/xonming/aioresilience.git
cd aioresilience

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install with all dependencies
pip install -e ".[all,dev]"

# Run tests
pytest

# Run tests with coverage
pytest --cov=aioresilience --cov-report=html

# Format code
black aioresilience/

# Type checking
mypy aioresilience/

# Linting
ruff check aioresilience/
----

=== Running Tests

[source,bash]
----
# Run all tests
pytest

# Run specific test file
pytest tests/test_circuit_breaker.py

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=aioresilience
----

== License

Copyright 2025 aioresilience contributors

Licensed under the MIT License.
You may obtain a copy of the License at

    https://opensource.org/licenses/MIT

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and limitations under the License.

== Acknowledgments

Special thanks to:

* https://github.com/mjpieters/aiolimiter[aiolimiter] for async rate limiting primitives

== Support

* *Documentation*: This README and Python docstrings
* *Issues*: https://github.com/xonming/aioresilience/issues[GitHub Issues]
* *Discussions*: https://github.com/xonming/aioresilience/discussions[GitHub Discussions]
* *PyPI*: https://pypi.org/project/aioresilience/[aioresilience on PyPI]

---

*Built with ❤️ for the Python asyncio community*
